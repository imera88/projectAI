{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1G3qhpcYBt0DdFKa1HQlRZo6UvV50LK8q","timestamp":1719465330597},{"file_id":"1R89fVZOCjwU9YMwWo4X-j14vwXiDuZ-q","timestamp":1669669680398}]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","source":["Estudiante:\n","\n","- Irving Mera\n","- Robinson Maqui"],"metadata":{"id":"EDHkvLhbQHI8"}},{"cell_type":"markdown","metadata":{"id":"n9PTfi4n7oS6"},"source":["# Práctico MABWISER\n","¿\n","Adaptado de los tutoriales disponibles en: https://github.com/fidelity/mabwiser/tree/master/examples"]},{"cell_type":"markdown","metadata":{"id":"WVOrpYsfi_kq"},"source":["## Reinforcement Learning\n","\n","Un agente de RL busca tomar acciones que logren maximizar la ganancia acumulativa.\n","\n","![RL setup](https://github.com/bamine/recsys-summer-school/raw/12e57cc4fd1cb26164d2beebf3ca29ebe2eab960/notebooks/images/rl-setup.png)\n","\n","\n","## Exploration vs. Exploitation\n","\n","Se busca encontrar un balance entre la exploración (decidir tomar una acción para ganar conocimiento) y exploitación (decidir la acción que se calcula que tendrá la mejor ganancia).\n","\n","![texto alternativo](https://miro.medium.com/max/1400/1*_5dltx4BcI8rRmCK2Sq_kw.png)"]},{"cell_type":"markdown","metadata":{"id":"5hhwKK1fOqq1"},"source":["## Importar paquetes necesarios"]},{"cell_type":"code","metadata":{"id":"OpCXt6tkYD_w","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1513b0bc-4557-4486-8a0d-8a8d3738edb2","executionInfo":{"status":"ok","timestamp":1719793503519,"user_tz":300,"elapsed":8022,"user":{"displayName":"Marcos Irving Mera Sanchez","userId":"00640408618716900836"}}},"source":["!pip install mabwiser\n","# -*- coding: utf-8 -*-\n","\n","import pandas as pd\n","from sklearn.preprocessing import StandardScaler\n","\n","from mabwiser.mab import MAB, LearningPolicy, NeighborhoodPolicy\n","import numpy as np\n","\n","import random\n","from mabwiser.simulator import Simulator\n","from scipy.spatial.distance import cdist\n","\n","from mabwiser.linear import _Linear\n","from mabwiser.mab import _EpsilonGreedy\n","from mabwiser.utils import argmax, Arm, Num, create_rng, _BaseRNG\n","\n","from typing import List\n","\n","from time import time"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: mabwiser in /usr/local/lib/python3.10/dist-packages (2.7.3)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from mabwiser) (1.4.2)\n","Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from mabwiser) (1.25.2)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from mabwiser) (2.0.3)\n","Requirement already satisfied: scikit-learn>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from mabwiser) (1.2.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from mabwiser) (1.11.4)\n","Requirement already satisfied: seaborn>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from mabwiser) (0.13.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.0->mabwiser) (3.5.0)\n","Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.10/dist-packages (from seaborn>=0.9.0->mabwiser) (3.7.1)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->mabwiser) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->mabwiser) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->mabwiser) (2024.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.9.0->mabwiser) (1.2.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.9.0->mabwiser) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.9.0->mabwiser) (4.53.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.9.0->mabwiser) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.9.0->mabwiser) (24.1)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.9.0->mabwiser) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.9.0->mabwiser) (3.1.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->mabwiser) (1.16.0)\n"]}]},{"cell_type":"markdown","metadata":{"id":"d_jkPlvD6NkV"},"source":["## Problema con contexto:\n","\n","Una plataforma de comercio virtual cuenta con 5 _ads_ y quiere determinar cuál de estos generará mayor recompensa para un usuario en específico. Para esto, se cuenta con la información histórica de las recompensas obtenidas por otros usuarios con distintos _ads_ junto con la información de la edad, la razón de clicks y si cada usuario es subscriptor o no."]},{"cell_type":"code","metadata":{"id":"q0fTQk08vaWC","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2d1ff7fa-263a-4f59-f044-8776937a5210","executionInfo":{"status":"ok","timestamp":1719793710383,"user_tz":300,"elapsed":490,"user":{"displayName":"Marcos Irving Mera Sanchez","userId":"00640408618716900836"}}},"source":["# Brazos, en este caso representan los 5 ads que se tienen disponibles para mostrarle a los usuarios.\n","ads = [1, 2, 3, 4, 5]\n","\n","# Se tiene la data histórica, estos pueden ser datos recopilados \"off-policy\", es decir, no necesariamente se recopilaron utilizando la política del\n","# agente que utilizaremos acá. En este caso, ad representa el brazo elegido, revenue la recompensa recolectada y age, click_rate y subscriber forman\n","# el contexto del usuario. Por ejemplo, el primer usuario tiene edad 22, un click_rate de 0.2 y es subscriptor, se le mostró el ad 1 y con esto se\n","# obtuvo una recompensa de 10.\n","train_df = pd.DataFrame({'ad': [1, 1, 1, 2, 4, 5, 3, 3, 2, 1, 4, 5, 3, 2, 5],\n","                         'revenues': [10, 17, 22, 9, 4, 20, 7, 8, 20, 9, 50, 5, 7, 12, 10],\n","                         'age': [22, 27, 39, 48, 21, 20, 19, 37, 52, 26, 18, 42, 55, 57, 38],\n","                         'click_rate': [0.2, 0.6, 0.99, 0.68, 0.15, 0.23, 0.75, 0.17,\n","                                        0.33, 0.65, 0.56, 0.22, 0.19, 0.11, 0.83],\n","                         'subscriber': [1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0]}\n","                        )\n","\n","# test_df representa el contexto de nuevos usuarios a los que se le desea entregar un ad.\n","test_df = pd.DataFrame({'age': [37, 52], 'click_rate': [0.5, 0.6], 'subscriber': [0, 1]})\n","# El revenue es la recompensa que se obtiene al entregarle un ad a estos usuarios. En un contexto real, esta recompensa sería entregada por el modelo\n","# después de haberle entregado el ad a los usuarios, pero para este tutorial la vamos a definir de antemano\n","test_df_revenue = pd.Series([7, 13])\n","\n","# Escalar el contexto del entrenamiento y testeo. Se deben escalar las características del contexto para evitar que una característica tenga más peso\n","# solo por la escala de sus valores.\n","scaler = StandardScaler()\n","print(train_df[['age', 'click_rate', 'subscriber']].values)\n","train = scaler.fit_transform(train_df[['age', 'click_rate', 'subscriber']].values.astype('float64'))\n","test = scaler.transform(test_df.values.astype('float64'))"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["[[22.    0.2   1.  ]\n"," [27.    0.6   0.  ]\n"," [39.    0.99  1.  ]\n"," [48.    0.68  0.  ]\n"," [21.    0.15  1.  ]\n"," [20.    0.23  0.  ]\n"," [19.    0.75  1.  ]\n"," [37.    0.17  1.  ]\n"," [52.    0.33  1.  ]\n"," [26.    0.65  0.  ]\n"," [18.    0.56  1.  ]\n"," [42.    0.22  1.  ]\n"," [55.    0.19  0.  ]\n"," [57.    0.11  1.  ]\n"," [38.    0.83  0.  ]]\n"]}]},{"cell_type":"markdown","source":["Se utilizará un algoritmo UCB1 para tomar decisiones. El algoritmo UCB1 estima la recompensa y elige la opción que tenga el límite superior de la predicción más elevado.\n"],"metadata":{"id":"FMZXwGE2ux2r"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YFOkn72Pwe2v","outputId":"7e2f0d53-8557-4cb1-92be-44a3815f7232","executionInfo":{"status":"ok","timestamp":1719793714740,"user_tz":300,"elapsed":832,"user":{"displayName":"Marcos Irving Mera Sanchez","userId":"00640408618716900836"}}},"source":["########################################################\n","# KNearest Neighborhood Policy with UCB1 Learning Policy\n","########################################################\n","\n","# Una recomendación no-paramétrica con contexto, requiere de una política de vecindad (Neighborhood Policy) para tomar decisiones basado en los\n","# usuarios más \"cercanos\" al usuario objetivo.\n","\n","# Política de vecindad KNearest con k=5 y política de aprendizaje ucb1 con alpha=1.25\n","knearest = MAB(arms=ads,\n","               learning_policy=LearningPolicy.UCB1(alpha=1.25),\n","               neighborhood_policy=NeighborhoodPolicy.KNearest(k=5))\n","\n","# El brazo aprende de las recompensas históricos.\n","knearest.fit(decisions=train_df['ad'], rewards=train_df['revenues'], contexts=train)\n","\n","# Se realiza la predicción.\n","prediction = knearest.predict(test)\n","\n","# Esperanza de recompensa para cada ad (este paso no es necesario, pero sirve para visualizar la toma de desiciones).\n","expectations = knearest.predict_expectations(test)\n","\n","# Resultado\n","print(\"KNearest: \", prediction, \" \", expectations)\n","assert(prediction == [5, 1])\n","\n","# Se actualiza la política de manera online con los nuevos resultados obtenidos (recordar que la recompensa se definió de antemano)\n","knearest.partial_fit(decisions=prediction, rewards=test_df_revenue, contexts=test)\n","\n","# Se puede agregar un nuevo brazo cuando sea necesario\n","knearest.add_arm(6)"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["KNearest:  [5, 1]   [{1: 14.5857953014744, 2: 11.242653222492628, 3: 0, 4: 0, 5: 16.5857953014744}, {1: 24.242653222492628, 2: 17.5857953014744, 3: 10.242653222492628, 4: 0, 5: 7.242653222492627}]\n"]}]},{"cell_type":"markdown","metadata":{"id":"cwpwQfVKe1BE"},"source":["### Actividad 1:\n","Suponga que a ambos usuarios, se les presenta el nuevo _ad_, $6$, obteniendo un revenue de $40$ para el primero y $10$ para el segundo.\n","\n","Es decir, utilice el método _partial\\_fit_ dándole como decisions el brazo [6], rewards=[40] y contexts=test, e imprima el resultado knearest.predict_expectations(test). Observe la predicción del brazo $6$ para cada usuario y si hubo cambios en la recompensa predicha para el resto de los brazos."]},{"cell_type":"markdown","source":["**Solución:**"],"metadata":{"id":"o-ti-e_gvewF"}},{"cell_type":"code","source":["# Inicialización de MAB con políticas de vecindad y aprendizaje\n","knearest = MAB(arms=ads,\n","               learning_policy=LearningPolicy.UCB1(alpha=1.25),\n","               neighborhood_policy=NeighborhoodPolicy.KNearest(k=5))\n","\n","# Entrenar con datos históricos definidos en el punto anterior\n","knearest.fit(decisions=train_df['ad'], rewards=train_df['revenues'], contexts=train)\n","\n","# Predicción antes de la actualización con el nuevo ad\n","predictions = knearest.predict(test)\n","expectations = knearest.predict_expectations(test)\n","\n","# Agregar el nuevo ad 6\n","knearest.add_arm(6)\n","#Se añade nueva decisión en base al ad 6\n","new_ad_decisions = [6, 6]\n","#Adición de ganancias\n","new_ad_rewards = [40, 10]\n","new_ad_contexts = test\n","\n","# Actualización del modelo con datos del nuevo ad\n","knearest.partial_fit(decisions=new_ad_decisions, rewards=new_ad_rewards, contexts=new_ad_contexts)\n","\n","# Predicción después de agregar el nuevo ad\n","updated_expectations = knearest.predict_expectations(test)\n","\n","print(\"Predicciones iniciales sin ad6: \", expectations)\n","print(\"Predicciones actualizadas con ad6: \", updated_expectations)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CtW2zCJpvafX","executionInfo":{"status":"ok","timestamp":1719795160621,"user_tz":300,"elapsed":498,"user":{"displayName":"Marcos Irving Mera Sanchez","userId":"00640408618716900836"}},"outputId":"1fb89570-da72-4577-b47e-a6291da21b5f"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Predicciones iniciales:  [{1: 14.5857953014744, 2: 11.242653222492628, 3: 0, 4: 0, 5: 16.5857953014744}, {1: 24.242653222492628, 2: 17.5857953014744, 3: 10.242653222492628, 4: 0, 5: 7.242653222492627}]\n","Predicciones actualizadas:  [{1: 14.5857953014744, 2: 11.242653222492628, 3: 0, 4: 0, 5: 12.242653222492628, 6: 42.24265322249263}, {1: 24.242653222492628, 2: 17.5857953014744, 3: 0, 4: 0, 5: 7.242653222492627, 6: 12.242653222492628}]\n"]}]},{"cell_type":"markdown","source":["**Comentarios:**\n","\n","En base a la nueva adición del anuncio 6, ha sido actualizado con una recompensa bastante alta (42, tomando como ganancia original 40), lo que refleja claramente un valor esperado alto ya que resulta muy atractivo el anuncio 6 para un usuario similar en futuras decisiones.\n","\n","Referente al segundo usuario, el incremento en la expectativa del anuncio 6 resulta un poco mayor (12, tomando como ganancia original 10), por lo que se indica que el modelo ha sido ajustado sus expectativas en base a la nueva información que toma en cuenta las decisiones pasadas y las recompensas relacionadas con otros anuncios que tienen un peso considerable en la decisión final."],"metadata":{"id":"FqHyieJ2uhMM"}},{"cell_type":"markdown","metadata":{"id":"JiV_AJDgHOIE"},"source":["## Problema sin contexto:\n","\n","El sitio ahora quiere elegir entre dos posibles opciones de diseño para su página principal. Para esto tiene datos históricos de la recompensa aportado por cada diseño, pero no cuenta con ningún dato acerca de los usuarios."]},{"cell_type":"code","metadata":{"id":"FqyI3tEyIHvF","executionInfo":{"status":"ok","timestamp":1719796694446,"user_tz":300,"elapsed":500,"user":{"displayName":"Marcos Irving Mera Sanchez","userId":"00640408618716900836"}}},"source":["# Brazos\n","options = [1, 2]\n","# Datos históricos (esta vez sin contexto del usuario), cada elemento representa una sesión de un usuario, ejemplo al primer usuario se le muestra el\n","# layout 1 y se obtiene un revenue 10.\n","layouts = [1, 1, 1, 2, 1, 2, 2, 1, 2, 1, 2, 2, 1, 2, 1]\n","revenues = [10, 17, 22, 9, 4, 0, 7, 8, 20, 9, 50, 5, 7, 12, 10]"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["En esta actividad, se utiliza la política Epsilon Greedy, la cual elige el brazo con una mayor esperanza de recompensa con probabilidad (1-epsilon).\n"],"metadata":{"id":"L2bNjNAYzO2d"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8xy6_yLHIU96","outputId":"4cff9d90-e67f-464a-b220-fa7eb3dc5c9a","executionInfo":{"status":"ok","timestamp":1719796697842,"user_tz":300,"elapsed":483,"user":{"displayName":"Marcos Irving Mera Sanchez","userId":"00640408618716900836"}}},"source":["###################################\n","# Epsilon Greedy Learning Policy\n","###################################\n","\n","# Se utiliza la política Epsilon Greedy con epsilon=15%, lo que significa que un 85% de las veces elegirá el brazo con mayor esperanza de recompensa.\n","# modificando o eliminando el valor de seed se pueden obtener resultados distintos.\n","greedy = MAB(arms=options,\n","             learning_policy=LearningPolicy.EpsilonGreedy(epsilon=0.15),\n","             seed=12345)\n","\n","# Se ajusta el modelo con los datos históricos\n","greedy.fit(decisions=layouts, rewards=revenues)\n","\n","# Se predice\n","prediction = greedy.predict()\n","\n","# Se almacenan los valores esperados\n","expectations = greedy.predict_expectations()\n","\n","print(\"Epsilon Greedy: \", prediction, \" \", expectations)\n","\n","# A medida que se generan nuevos resultados, se almacenan las recompensas y se pueden usar para actualizar el modelo.\n","\n","additional_layouts = [1, 2, 1, 2]\n","additional_revenues = [25, 2, 19, 1]\n","\n","greedy.partial_fit(additional_layouts, additional_revenues)\n","\n","prediction = greedy.predict()\n","expectations = greedy.predict_expectations()\n","\n","print(\"Epsilon Greedy: \", prediction, \" \", expectations)\n","# Observamos que, al actualizar el modelo con recompensas altas para la opción 1 y bajas para la opción 2, la opción 1 pasa a ser la opción con mayor\n","# valor esperado"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Epsilon Greedy:  2   {1: 10.875, 2: 14.714285714285714}\n","Epsilon Greedy:  1   {1: 13.1, 2: 11.777777777777779}\n"]}]},{"cell_type":"markdown","metadata":{"id":"f8SYnQm7filh"},"source":["### Actividad 2:\n","Suponga ahora que se muestran ambos anuncios y en ambos, se obtiene una recompensa de $20$, es decir, genere un vector de layouts $[1,2]$ y recompensas $[20,20]$ y con esto invoque a la función partial\\_fit e imprima lo entregado por _predict\\_expectations()_. Qué valor esperado aumentó más? Por qué?"]},{"cell_type":"markdown","source":["**Solución:**"],"metadata":{"id":"_EHCggjj3yMG"}},{"cell_type":"code","metadata":{"id":"yNPDzT7mIV6r","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1719797276683,"user_tz":300,"elapsed":544,"user":{"displayName":"Marcos Irving Mera Sanchez","userId":"00640408618716900836"}},"outputId":"6818cfa4-19d6-417c-bda1-f351b08b1d24"},"source":["# Inicialización del modelo Epsilon Greedy\n","greedy = MAB(arms=options,\n","             learning_policy=LearningPolicy.EpsilonGreedy(epsilon=0.15),\n","             seed=12345)  # Semilla para reproducibilidad\n","\n","# Ajuste del modelo con los datos históricos\n","greedy.fit(decisions=layouts, rewards=revenues)\n","\n","# Predicción inicial y expectativas de recompensa\n","initial_prediction = greedy.predict()\n","initial_expectations = greedy.predict_expectations()\n","\n","print(\"Epsilon Greedy antes de la actualización: \", initial_prediction, \" \", initial_expectations)\n","\n","# Actualización del modelo con nuevos datos donde ambas opciones obtienen la misma recompensa\n","new_layouts = [1, 2]  # Nuevas decisiones\n","new_revenues = [20, 20]  # Nuevas recompensas\n","\n","# Actualización del modelo con los nuevos resultados\n","greedy.partial_fit(new_layouts, new_revenues)\n","\n","# Predicción y expectativas después de la actualización\n","updated_prediction = greedy.predict()\n","updated_expectations = greedy.predict_expectations()\n","\n","print(\"Epsilon Greedy después de la actualización: \", updated_prediction, \" \", updated_expectations)"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Epsilon Greedy antes de la actualización:  2   {1: 10.875, 2: 14.714285714285714}\n","Epsilon Greedy después de la actualización:  2   {1: 11.88888888888889, 2: 15.375}\n"]}]},{"cell_type":"markdown","source":["**Comentario:**\n","\n","Se observa que ambos valores aumentaron su valor esperado realizado al entrenamiento de Epsilon Greedy antes de la actualización. Esto es un reflejo directo del aprendizaje del modelo, que incorpora las nuevas recompensas en su estimación de la recompensa promedio esperada para cada opción.\n","Asimismo, se observa que la opción 2 sigue viéndose como la más ventajosa debido a que ha recibido recompensas más altas en comparación a la opción 1, lo que ha establecido una expectativa más alta que se mantiene incluso después de la actualización.\n","\n","La política Epsilon Greedy utiliza un parámetro epsilon del 15%, lo que significa que el 85% de las veces, el modelo optará por la opción con la mayor expectativa de recompensa (en este caso, la opción 2), mientras que el 15% de las veces explorará aleatoriamente entre las opciones. Este enfoque permite que el modelo ajuste y refina sus expectativas a medida que recibe más información, sin quedar atrapado en una sola opción desde el inicio; lo que demuestra su capacidad para ajustar de forma dinámica las expectativas de recompensas basadas en nueva información en relación hacia el anterior ejercicio donde nos basamos la entrega de la predicción en base a un contexto definido anterior a la predicción."],"metadata":{"id":"khbiv7KS5_hW"}},{"cell_type":"markdown","source":["# MAB customizado"],"metadata":{"id":"LPnuSwsgwebe"}},{"cell_type":"markdown","source":["Supongamos que tenemos un escenario en el que tomar una decisión tiene un costo fijo adicional. Para esto se define un margen de costo al cuál se está dispuesto a incurrir, y en caso de que la recompensa del mejor brazo no supere este costo, no se podrá tomar la opción con mejor recompensa.\n","\n","Para esto, importamos la clase EpsilonGreedy y modificamos su función predict() para que refleje esta nueva condición."],"metadata":{"id":"bwCN6UVCwj8m"}},{"cell_type":"code","source":["class CustomGreedy(_EpsilonGreedy):\n","\n","    def __init__(self, rng: _BaseRNG, arms: List[Arm], n_jobs: int, margin: Num, backend: str = None):\n","        # Se inicializa la clase _EpsilonGreedy\n","        super().__init__(rng, arms, n_jobs, backend)\n","        # Se define el margen del costo\n","        self.margin = margin\n","\n","    # Se sobreescribe el método predict\n","    def predict(self, contexts: np.ndarray=None):\n","\n","        # Se generan las esperanzas basado en la función original\n","        arm_to_exp = super().predict_expectations()\n","\n","        # Se encuentra el brazo con mayor esperanza\n","        max_arm = argmax(arm_to_exp)\n","        max_exp = arm_to_exp[max_arm]\n","        print(\"Max arm: \", max_arm, \" with expectation: \", max_exp)\n","\n","        # El segundo mayor\n","        del arm_to_exp[max_arm]\n","        next_max_arm = argmax(arm_to_exp)\n","        next_max_exp = arm_to_exp[next_max_arm]\n","        print(\"Next max arm: \", next_max_arm, \" with expectation: \", next_max_exp)\n","\n","        # Se encuentra el regret entre el mejor brazo y el segundo mejor\n","        regret = max_exp - next_max_exp\n","        print(\"Regret: \", regret, \" margin: \", self.margin)\n","\n","        # Se devuelve el brazo con mejor esperanza\n","        # solo si el regret supera el margen definido\n","        return max_arm if regret >= self.margin else next_max_arm"],"metadata":{"id":"Cc5-JCbGxlff"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["rng = create_rng(123456)\n","\n","options = [1, 2]\n","\n","layouts = np.array([1, 1, 1, 2, 1, 2, 2, 1, 2, 1, 2, 2, 1, 2, 1])\n","revenues = np.array([10, 17, 22, 9, 4, 0, 7, 8, 20, 9, 50, 5, 7, 12, 10])\n","\n","# Se genera un mab de nuestra clase CustomGreedy con un margen de costo moderado\n","greedy = CustomGreedy(rng, options, 1, 3.0)\n","greedy.fit(decisions=layouts, rewards=revenues)\n","prediction = greedy.predict()\n","expectations = greedy.predict_expectations()\n","\n","print(\"Custom Margin 3 Greedy: \", prediction, \" \", expectations, \"\\n\")\n","\n","# Observamos que el Regret supera el costo, por lo que se entrega el brazo con mejor esperanza\n","\n","# Esta vez generamos un nuevo CustomGreedy con un margen elevado\n","greedy = CustomGreedy(rng, options, 1, 5)\n","greedy.fit(decisions=layouts, rewards=revenues)\n","prediction = greedy.predict()\n","expectations = greedy.predict_expectations()\n","print(\"Custom Margin 5 Greedy: \", prediction, \" \", expectations, \"\\n\")\n","\n","# En este caso, el costo es mayor al regret, por lo que se entrega el segundo \"mejor\" brazo"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LP_7SQC0yhQK","outputId":"cbf18831-6bd7-46bf-a6ad-8c249b361c3b","executionInfo":{"status":"ok","timestamp":1669679104501,"user_tz":180,"elapsed":256,"user":{"displayName":"NICOLAS SUMONTE FUENZALIDA","userId":"00746292366416689484"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Max arm:  2  with expectation:  14.714285714285714\n","Next max arm:  1  with expectation:  10.875\n","Regret:  3.8392857142857135  margin:  3.0\n","Custom Margin 3 Greedy:  2   {1: 10.875, 2: 14.714285714285714} \n","\n","Max arm:  1  with expectation:  0.9552527396157818\n","Next max arm:  2  with expectation:  0.906050936603124\n","Regret:  0.04920180301265786  margin:  5\n","Custom Margin 5 Greedy:  2   {1: 10.875, 2: 14.714285714285714} \n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"0RkfsRP_J8AD"},"source":["## Simulación:\n","\n","Vamos a simular multi-armed bandits para un dataset aleatorio con $1000$ entradas de datos y comparar los rendimientos de cada política de aprendizaje:"]},{"cell_type":"code","metadata":{"id":"PL4CRpsgU64C"},"source":["size = 1000\n","\n","# Se generan 1000 datos aleatorios de decisiones y recompensas (historial)\n","random.seed(12345)\n","\n","decisions = [random.randint(0, 2) for _ in range(size)]\n","rewards = [random.randint(0, 1000) for _ in range(size)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DtgXBJ0aU8e3"},"source":["####################################\n","# Different Bandits for Simulation\n","####################################\n","\n","def binarize(decision, reward): # La política Thompsom Sampling requiere que las recompensas sean binarias.\n","    return reward >= 500\n","\n","n_jobs=2\n","\n","context_free_mabs = [('Random', MAB([0, 1], LearningPolicy.Random(), n_jobs=n_jobs)),\n","                     ('UCB1', MAB([0, 1], LearningPolicy.UCB1(1), n_jobs=n_jobs)),\n","                     ('ThompsonSampling', MAB([0, 1], LearningPolicy.ThompsonSampling(binarize), n_jobs=n_jobs)),\n","                     ('EpsilonGreedy', MAB([0, 1], LearningPolicy.EpsilonGreedy(epsilon=.15), n_jobs=n_jobs)),\n","                     ('Softmax', MAB([0, 1], LearningPolicy.Softmax(), n_jobs=n_jobs))]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pE2uopohVYST","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"30ed6327-57b5-44c0-dafc-aa4c82d803c4","executionInfo":{"status":"ok","timestamp":1669679306146,"user_tz":180,"elapsed":1461,"user":{"displayName":"NICOLAS SUMONTE FUENZALIDA","userId":"00746292366416689484"}}},"source":["####################################\n","# Context-Free Simulation\n","####################################\n","start = time()\n","sim = Simulator(context_free_mabs, decisions, rewards, contexts=None,\n","                scaler=None, test_size=0.5, is_ordered=False, batch_size=0, seed=123456)\n","\n","# Como vamos a simular problemas sin contexto, los valores de context y scaler son None. test_size indica que un 50% de los datos se utilizarán como\n","# test. is_order indica que el train_test split es aleatorio. batch_size significa que en la etapa de testeo, cada resultado nuevo actualiza de\n","# manera online la política. Si se pusiera batch_size=50, se actualizaría la política cada 50 resultados.\n","\n","sim.run()\n","end = time()\n","\n","runtime = (end - start) / 60\n","print('Complete', str(runtime) + ' minutes')\n","print('\\n')\n","\n","for mab_name, mab in sim.bandits:\n","    #Por cada mab se imprimen los resultados en el peor caso, caso promedio y mejor caso.\n","    print(mab_name)\n","\n","    print('Worst Case Scenario:', sim.bandit_to_arm_to_stats_min[mab_name])\n","    print('Average Case Scenario:', sim.bandit_to_arm_to_stats_avg[mab_name])\n","    print('Best Case Scenario:', sim.bandit_to_arm_to_stats_max[mab_name])\n","\n","    print('\\n\\n')\n","\n","sim.plot('avg', True)\n","# Vamos a plotear el escenario promedio para todos los bandidos utilizados. El parámetro True indica que en el plot se va a plotear por separado cada \"brazo\""],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:root:Simulation Parameters\n","2022-11-28 23:48:24,486 INFO Simulation Parameters\n","2022-11-28 23:48:24,486 INFO Simulation Parameters\n","INFO:root:\t bandits: [('Random', <mabwiser.mab.MAB object at 0x7f8ed4e7f550>), ('UCB1', <mabwiser.mab.MAB object at 0x7f8ed4e7f610>), ('ThompsonSampling', <mabwiser.mab.MAB object at 0x7f8ed4e7fdd0>), ('EpsilonGreedy', <mabwiser.mab.MAB object at 0x7f8ed49186d0>), ('Softmax', <mabwiser.mab.MAB object at 0x7f8ed5f399d0>)]\n","2022-11-28 23:48:24,497 INFO \t bandits: [('Random', <mabwiser.mab.MAB object at 0x7f8ed4e7f550>), ('UCB1', <mabwiser.mab.MAB object at 0x7f8ed4e7f610>), ('ThompsonSampling', <mabwiser.mab.MAB object at 0x7f8ed4e7fdd0>), ('EpsilonGreedy', <mabwiser.mab.MAB object at 0x7f8ed49186d0>), ('Softmax', <mabwiser.mab.MAB object at 0x7f8ed5f399d0>)]\n","2022-11-28 23:48:24,497 INFO \t bandits: [('Random', <mabwiser.mab.MAB object at 0x7f8ed4e7f550>), ('UCB1', <mabwiser.mab.MAB object at 0x7f8ed4e7f610>), ('ThompsonSampling', <mabwiser.mab.MAB object at 0x7f8ed4e7fdd0>), ('EpsilonGreedy', <mabwiser.mab.MAB object at 0x7f8ed49186d0>), ('Softmax', <mabwiser.mab.MAB object at 0x7f8ed5f399d0>)]\n","INFO:root:\t scaler: None\n","2022-11-28 23:48:24,501 INFO \t scaler: None\n","2022-11-28 23:48:24,501 INFO \t scaler: None\n","INFO:root:\t test_size: 0.5\n","2022-11-28 23:48:24,505 INFO \t test_size: 0.5\n","2022-11-28 23:48:24,505 INFO \t test_size: 0.5\n","INFO:root:\t is_ordered: False\n","2022-11-28 23:48:24,509 INFO \t is_ordered: False\n","2022-11-28 23:48:24,509 INFO \t is_ordered: False\n","INFO:root:\t batch_size: 0\n","2022-11-28 23:48:24,515 INFO \t batch_size: 0\n","2022-11-28 23:48:24,515 INFO \t batch_size: 0\n","INFO:root:\t evaluator: <function default_evaluator at 0x7f8ed6849950>\n","2022-11-28 23:48:24,528 INFO \t evaluator: <function default_evaluator at 0x7f8ed6849950>\n","2022-11-28 23:48:24,528 INFO \t evaluator: <function default_evaluator at 0x7f8ed6849950>\n","INFO:root:\t seed: 123456\n","2022-11-28 23:48:24,533 INFO \t seed: 123456\n","2022-11-28 23:48:24,533 INFO \t seed: 123456\n","INFO:root:\t is_quick: False\n","2022-11-28 23:48:24,539 INFO \t is_quick: False\n","2022-11-28 23:48:24,539 INFO \t is_quick: False\n","INFO:root:\t log_file: None\n","2022-11-28 23:48:24,543 INFO \t log_file: None\n","2022-11-28 23:48:24,543 INFO \t log_file: None\n","INFO:root:\t format: %(asctime)s %(levelname)s %(message)s\n","2022-11-28 23:48:24,548 INFO \t format: %(asctime)s %(levelname)s %(message)s\n","2022-11-28 23:48:24,548 INFO \t format: %(asctime)s %(levelname)s %(message)s\n","INFO:root:\n","\n","2022-11-28 23:48:24,552 INFO \n","\n","2022-11-28 23:48:24,552 INFO \n","\n","INFO:root:Total Stats\n","2022-11-28 23:48:24,556 INFO Total Stats\n","2022-11-28 23:48:24,556 INFO Total Stats\n","INFO:root:{0: {'count': 360, 'sum': 192273, 'min': 1, 'max': 997, 'mean': 534.0916666666667, 'std': 297.05323117713726}, 1: {'count': 331, 'sum': 172828, 'min': 1, 'max': 1000, 'mean': 522.1389728096676, 'std': 296.62448214129495}}\n","2022-11-28 23:48:24,560 INFO {0: {'count': 360, 'sum': 192273, 'min': 1, 'max': 997, 'mean': 534.0916666666667, 'std': 297.05323117713726}, 1: {'count': 331, 'sum': 172828, 'min': 1, 'max': 1000, 'mean': 522.1389728096676, 'std': 296.62448214129495}}\n","2022-11-28 23:48:24,560 INFO {0: {'count': 360, 'sum': 192273, 'min': 1, 'max': 997, 'mean': 534.0916666666667, 'std': 297.05323117713726}, 1: {'count': 331, 'sum': 172828, 'min': 1, 'max': 1000, 'mean': 522.1389728096676, 'std': 296.62448214129495}}\n","INFO:root:\n","\n","2022-11-28 23:48:24,564 INFO \n","\n","2022-11-28 23:48:24,564 INFO \n","\n","INFO:root:Train/Test Split\n","2022-11-28 23:48:24,568 INFO Train/Test Split\n","2022-11-28 23:48:24,568 INFO Train/Test Split\n","INFO:root:Train size: 500\n","2022-11-28 23:48:24,575 INFO Train size: 500\n","2022-11-28 23:48:24,575 INFO Train size: 500\n","INFO:root:Test size: 500\n","2022-11-28 23:48:24,579 INFO Test size: 500\n","2022-11-28 23:48:24,579 INFO Test size: 500\n","INFO:root:\n","\n","2022-11-28 23:48:24,583 INFO \n","\n","2022-11-28 23:48:24,583 INFO \n","\n","INFO:root:Train Stats\n","2022-11-28 23:48:24,587 INFO Train Stats\n","2022-11-28 23:48:24,587 INFO Train Stats\n","INFO:root:{0: {'count': 175, 'sum': 92110, 'min': 1, 'max': 996, 'mean': 526.3428571428572, 'std': 296.87018258175146}, 1: {'count': 170, 'sum': 90623, 'min': 16, 'max': 1000, 'mean': 533.0764705882353, 'std': 300.1780452633595}}\n","2022-11-28 23:48:24,591 INFO {0: {'count': 175, 'sum': 92110, 'min': 1, 'max': 996, 'mean': 526.3428571428572, 'std': 296.87018258175146}, 1: {'count': 170, 'sum': 90623, 'min': 16, 'max': 1000, 'mean': 533.0764705882353, 'std': 300.1780452633595}}\n","2022-11-28 23:48:24,591 INFO {0: {'count': 175, 'sum': 92110, 'min': 1, 'max': 996, 'mean': 526.3428571428572, 'std': 296.87018258175146}, 1: {'count': 170, 'sum': 90623, 'min': 16, 'max': 1000, 'mean': 533.0764705882353, 'std': 300.1780452633595}}\n","INFO:root:\n","\n","2022-11-28 23:48:24,621 INFO \n","\n","2022-11-28 23:48:24,621 INFO \n","\n","INFO:root:Test Stats\n","2022-11-28 23:48:24,627 INFO Test Stats\n","2022-11-28 23:48:24,627 INFO Test Stats\n","INFO:root:{0: {'count': 185, 'sum': 100163, 'min': 3, 'max': 997, 'mean': 541.4216216216216, 'std': 297.04029314004134}, 1: {'count': 161, 'sum': 82205, 'min': 1, 'max': 997, 'mean': 510.5900621118012, 'std': 292.38170559149626}}\n","2022-11-28 23:48:24,632 INFO {0: {'count': 185, 'sum': 100163, 'min': 3, 'max': 997, 'mean': 541.4216216216216, 'std': 297.04029314004134}, 1: {'count': 161, 'sum': 82205, 'min': 1, 'max': 997, 'mean': 510.5900621118012, 'std': 292.38170559149626}}\n","2022-11-28 23:48:24,632 INFO {0: {'count': 185, 'sum': 100163, 'min': 3, 'max': 997, 'mean': 541.4216216216216, 'std': 297.04029314004134}, 1: {'count': 161, 'sum': 82205, 'min': 1, 'max': 997, 'mean': 510.5900621118012, 'std': 292.38170559149626}}\n","INFO:root:\n","\n","2022-11-28 23:48:24,638 INFO \n","\n","2022-11-28 23:48:24,638 INFO \n","\n","INFO:root:Training Bandits\n","2022-11-28 23:48:24,644 INFO Training Bandits\n","2022-11-28 23:48:24,644 INFO Training Bandits\n","INFO:root:Random trained\n","2022-11-28 23:48:24,649 INFO Random trained\n","2022-11-28 23:48:24,649 INFO Random trained\n","INFO:root:UCB1 trained\n","2022-11-28 23:48:24,817 INFO UCB1 trained\n","2022-11-28 23:48:24,817 INFO UCB1 trained\n","INFO:root:ThompsonSampling trained\n","2022-11-28 23:48:24,928 INFO ThompsonSampling trained\n","2022-11-28 23:48:24,928 INFO ThompsonSampling trained\n","INFO:root:EpsilonGreedy trained\n","2022-11-28 23:48:25,039 INFO EpsilonGreedy trained\n","2022-11-28 23:48:25,039 INFO EpsilonGreedy trained\n","INFO:root:Softmax trained\n","2022-11-28 23:48:25,153 INFO Softmax trained\n","2022-11-28 23:48:25,153 INFO Softmax trained\n","INFO:root:\n","\n","2022-11-28 23:48:25,157 INFO \n","\n","2022-11-28 23:48:25,157 INFO \n","\n","INFO:root:Testing Bandits\n","2022-11-28 23:48:25,163 INFO Testing Bandits\n","2022-11-28 23:48:25,163 INFO Testing Bandits\n","INFO:root:Chunk 1 out of 1\n","2022-11-28 23:48:25,174 INFO Chunk 1 out of 1\n","2022-11-28 23:48:25,174 INFO Chunk 1 out of 1\n","INFO:root:Random confusion matrix: [array([[92, 93,  0],\n","       [87, 74,  0],\n","       [74, 80,  0]])]\n","2022-11-28 23:48:25,229 INFO Random confusion matrix: [array([[92, 93,  0],\n","       [87, 74,  0],\n","       [74, 80,  0]])]\n","2022-11-28 23:48:25,229 INFO Random confusion matrix: [array([[92, 93,  0],\n","       [87, 74,  0],\n","       [74, 80,  0]])]\n","INFO:root:Random minimum analysis {0: {'count': 253, 'sum': 50565, 'min': 1, 'max': 997, 'mean': 199.86166007905138, 'std': 317.52001062397824}, 1: {'count': 247, 'sum': 36159, 'min': 9, 'max': 992, 'mean': 146.39271255060729, 'std': 257.06731980252925}}\n","2022-11-28 23:48:25,240 INFO Random minimum analysis {0: {'count': 253, 'sum': 50565, 'min': 1, 'max': 997, 'mean': 199.86166007905138, 'std': 317.52001062397824}, 1: {'count': 247, 'sum': 36159, 'min': 9, 'max': 992, 'mean': 146.39271255060729, 'std': 257.06731980252925}}\n","2022-11-28 23:48:25,240 INFO Random minimum analysis {0: {'count': 253, 'sum': 50565, 'min': 1, 'max': 997, 'mean': 199.86166007905138, 'std': 317.52001062397824}, 1: {'count': 247, 'sum': 36159, 'min': 9, 'max': 992, 'mean': 146.39271255060729, 'std': 257.06731980252925}}\n","INFO:root:Random average analysis {0: {'count': 253, 'sum': 135145.2, 'min': 10.0, 'max': 997.0, 'mean': 534.1707509881423, 'std': 178.1032292626969}, 1: {'count': 247, 'sum': 125613.22941176473, 'min': 9.0, 'max': 992.0, 'mean': 508.55558466301505, 'std': 166.55558262639775}}\n","2022-11-28 23:48:25,253 INFO Random average analysis {0: {'count': 253, 'sum': 135145.2, 'min': 10.0, 'max': 997.0, 'mean': 534.1707509881423, 'std': 178.1032292626969}, 1: {'count': 247, 'sum': 125613.22941176473, 'min': 9.0, 'max': 992.0, 'mean': 508.55558466301505, 'std': 166.55558262639775}}\n","2022-11-28 23:48:25,253 INFO Random average analysis {0: {'count': 253, 'sum': 135145.2, 'min': 10.0, 'max': 997.0, 'mean': 534.1707509881423, 'std': 178.1032292626969}, 1: {'count': 247, 'sum': 125613.22941176473, 'min': 9.0, 'max': 992.0, 'mean': 508.55558466301505, 'std': 166.55558262639775}}\n","INFO:root:Random maximum analysis {0: {'count': 253, 'sum': 210760, 'min': 10, 'max': 997, 'mean': 833.0434782608696, 'std': 279.43599691254957}, 1: {'count': 247, 'sum': 206391, 'min': 9, 'max': 1000, 'mean': 835.591093117409, 'std': 299.2114853362161}}\n","2022-11-28 23:48:25,259 INFO Random maximum analysis {0: {'count': 253, 'sum': 210760, 'min': 10, 'max': 997, 'mean': 833.0434782608696, 'std': 279.43599691254957}, 1: {'count': 247, 'sum': 206391, 'min': 9, 'max': 1000, 'mean': 835.591093117409, 'std': 299.2114853362161}}\n","2022-11-28 23:48:25,259 INFO Random maximum analysis {0: {'count': 253, 'sum': 210760, 'min': 10, 'max': 997, 'mean': 833.0434782608696, 'std': 279.43599691254957}, 1: {'count': 247, 'sum': 206391, 'min': 9, 'max': 1000, 'mean': 835.591093117409, 'std': 299.2114853362161}}\n","INFO:root:UCB1 confusion matrix: [array([[  0, 185,   0],\n","       [  0, 161,   0],\n","       [  0, 154,   0]])]\n","2022-11-28 23:48:25,268 INFO UCB1 confusion matrix: [array([[  0, 185,   0],\n","       [  0, 161,   0],\n","       [  0, 154,   0]])]\n","2022-11-28 23:48:25,268 INFO UCB1 confusion matrix: [array([[  0, 185,   0],\n","       [  0, 161,   0],\n","       [  0, 154,   0]])]\n","INFO:root:UCB1 minimum analysis {0: {'count': 0, 'sum': nan, 'min': nan, 'max': nan, 'mean': nan, 'std': nan}, 1: {'count': 500, 'sum': 87629, 'min': 1, 'max': 997, 'mean': 175.258, 'std': 284.4840020739303}}\n","2022-11-28 23:48:25,277 INFO UCB1 minimum analysis {0: {'count': 0, 'sum': nan, 'min': nan, 'max': nan, 'mean': nan, 'std': nan}, 1: {'count': 500, 'sum': 87629, 'min': 1, 'max': 997, 'mean': 175.258, 'std': 284.4840020739303}}\n","2022-11-28 23:48:25,277 INFO UCB1 minimum analysis {0: {'count': 0, 'sum': nan, 'min': nan, 'max': nan, 'mean': nan, 'std': nan}, 1: {'count': 500, 'sum': 87629, 'min': 1, 'max': 997, 'mean': 175.258, 'std': 284.4840020739303}}\n","INFO:root:UCB1 average analysis {0: {'count': 0, 'sum': nan, 'min': nan, 'max': nan, 'mean': nan, 'std': nan}, 1: {'count': 500, 'sum': 262917.9235294118, 'min': 1.0, 'max': 997.0, 'mean': 525.8358470588236, 'std': 166.24446720841726}}\n","2022-11-28 23:48:25,281 INFO UCB1 average analysis {0: {'count': 0, 'sum': nan, 'min': nan, 'max': nan, 'mean': nan, 'std': nan}, 1: {'count': 500, 'sum': 262917.9235294118, 'min': 1.0, 'max': 997.0, 'mean': 525.8358470588236, 'std': 166.24446720841726}}\n","2022-11-28 23:48:25,281 INFO UCB1 average analysis {0: {'count': 0, 'sum': nan, 'min': nan, 'max': nan, 'mean': nan, 'std': nan}, 1: {'count': 500, 'sum': 262917.9235294118, 'min': 1.0, 'max': 997.0, 'mean': 525.8358470588236, 'std': 166.24446720841726}}\n","INFO:root:UCB1 maximum analysis {0: {'count': 0, 'sum': nan, 'min': nan, 'max': nan, 'mean': nan, 'std': nan}, 1: {'count': 500, 'sum': 421205, 'min': 1, 'max': 1000, 'mean': 842.41, 'std': 282.5213937032026}}\n","2022-11-28 23:48:25,285 INFO UCB1 maximum analysis {0: {'count': 0, 'sum': nan, 'min': nan, 'max': nan, 'mean': nan, 'std': nan}, 1: {'count': 500, 'sum': 421205, 'min': 1, 'max': 1000, 'mean': 842.41, 'std': 282.5213937032026}}\n","2022-11-28 23:48:25,285 INFO UCB1 maximum analysis {0: {'count': 0, 'sum': nan, 'min': nan, 'max': nan, 'mean': nan, 'std': nan}, 1: {'count': 500, 'sum': 421205, 'min': 1, 'max': 1000, 'mean': 842.41, 'std': 282.5213937032026}}\n","INFO:root:ThompsonSampling confusion matrix: [array([[ 72, 113,   0],\n","       [ 80,  81,   0],\n","       [ 67,  87,   0]])]\n","2022-11-28 23:48:25,290 INFO ThompsonSampling confusion matrix: [array([[ 72, 113,   0],\n","       [ 80,  81,   0],\n","       [ 67,  87,   0]])]\n","2022-11-28 23:48:25,290 INFO ThompsonSampling confusion matrix: [array([[ 72, 113,   0],\n","       [ 80,  81,   0],\n","       [ 67,  87,   0]])]\n","INFO:root:ThompsonSampling minimum analysis {0: {'count': 219, 'sum': 39891, 'min': 1, 'max': 997, 'mean': 182.15068493150685, 'std': 310.8620362870862}, 1: {'count': 281, 'sum': 43647, 'min': 12, 'max': 992, 'mean': 155.32740213523132, 'std': 271.9760945573872}}\n","2022-11-28 23:48:25,297 INFO ThompsonSampling minimum analysis {0: {'count': 219, 'sum': 39891, 'min': 1, 'max': 997, 'mean': 182.15068493150685, 'std': 310.8620362870862}, 1: {'count': 281, 'sum': 43647, 'min': 12, 'max': 992, 'mean': 155.32740213523132, 'std': 271.9760945573872}}\n","2022-11-28 23:48:25,297 INFO ThompsonSampling minimum analysis {0: {'count': 219, 'sum': 39891, 'min': 1, 'max': 997, 'mean': 182.15068493150685, 'std': 310.8620362870862}, 1: {'count': 281, 'sum': 43647, 'min': 12, 'max': 992, 'mean': 155.32740213523132, 'std': 271.9760945573872}}\n","INFO:root:ThompsonSampling average analysis {0: {'count': 219, 'sum': 117116.40000000002, 'min': 10.0, 'max': 997.0, 'mean': 534.778082191781, 'std': 172.57467835964198}, 1: {'count': 281, 'sum': 147062.29411764705, 'min': 12.0, 'max': 992.0, 'mean': 523.3533598492778, 'std': 162.09026049002696}}\n","2022-11-28 23:48:25,301 INFO ThompsonSampling average analysis {0: {'count': 219, 'sum': 117116.40000000002, 'min': 10.0, 'max': 997.0, 'mean': 534.778082191781, 'std': 172.57467835964198}, 1: {'count': 281, 'sum': 147062.29411764705, 'min': 12.0, 'max': 992.0, 'mean': 523.3533598492778, 'std': 162.09026049002696}}\n","2022-11-28 23:48:25,301 INFO ThompsonSampling average analysis {0: {'count': 219, 'sum': 117116.40000000002, 'min': 10.0, 'max': 997.0, 'mean': 534.778082191781, 'std': 172.57467835964198}, 1: {'count': 281, 'sum': 147062.29411764705, 'min': 12.0, 'max': 992.0, 'mean': 523.3533598492778, 'std': 162.09026049002696}}\n","INFO:root:ThompsonSampling maximum analysis {0: {'count': 219, 'sum': 186156, 'min': 10, 'max': 997, 'mean': 850.027397260274, 'std': 270.44515844118195}, 1: {'count': 281, 'sum': 240447, 'min': 12, 'max': 1000, 'mean': 855.6832740213523, 'std': 278.32588500024013}}\n","2022-11-28 23:48:25,305 INFO ThompsonSampling maximum analysis {0: {'count': 219, 'sum': 186156, 'min': 10, 'max': 997, 'mean': 850.027397260274, 'std': 270.44515844118195}, 1: {'count': 281, 'sum': 240447, 'min': 12, 'max': 1000, 'mean': 855.6832740213523, 'std': 278.32588500024013}}\n","2022-11-28 23:48:25,305 INFO ThompsonSampling maximum analysis {0: {'count': 219, 'sum': 186156, 'min': 10, 'max': 997, 'mean': 850.027397260274, 'std': 270.44515844118195}, 1: {'count': 281, 'sum': 240447, 'min': 12, 'max': 1000, 'mean': 855.6832740213523, 'std': 278.32588500024013}}\n","INFO:root:EpsilonGreedy confusion matrix: [array([[ 15, 170,   0],\n","       [ 16, 145,   0],\n","       [ 13, 141,   0]])]\n","2022-11-28 23:48:25,310 INFO EpsilonGreedy confusion matrix: [array([[ 15, 170,   0],\n","       [ 16, 145,   0],\n","       [ 13, 141,   0]])]\n","2022-11-28 23:48:25,310 INFO EpsilonGreedy confusion matrix: [array([[ 15, 170,   0],\n","       [ 16, 145,   0],\n","       [ 13, 141,   0]])]\n","INFO:root:EpsilonGreedy minimum analysis {0: {'count': 44, 'sum': 7538, 'min': 1, 'max': 869, 'mean': 171.3181818181818, 'std': 274.8274477432301}, 1: {'count': 456, 'sum': 78663, 'min': 1, 'max': 992, 'mean': 172.5065789473684, 'std': 281.7247006818204}}\n","2022-11-28 23:48:25,316 INFO EpsilonGreedy minimum analysis {0: {'count': 44, 'sum': 7538, 'min': 1, 'max': 869, 'mean': 171.3181818181818, 'std': 274.8274477432301}, 1: {'count': 456, 'sum': 78663, 'min': 1, 'max': 992, 'mean': 172.5065789473684, 'std': 281.7247006818204}}\n","2022-11-28 23:48:25,316 INFO EpsilonGreedy minimum analysis {0: {'count': 44, 'sum': 7538, 'min': 1, 'max': 869, 'mean': 171.3181818181818, 'std': 274.8274477432301}, 1: {'count': 456, 'sum': 78663, 'min': 1, 'max': 992, 'mean': 172.5065789473684, 'std': 281.7247006818204}}\n","INFO:root:EpsilonGreedy average analysis {0: {'count': 44, 'sum': 22772.942857142854, 'min': 93.0, 'max': 869.0, 'mean': 517.5668831168831, 'std': 139.9869507492786}, 1: {'count': 456, 'sum': 239473.78235294123, 'min': 1.0, 'max': 992.0, 'mean': 525.1618034055729, 'std': 164.21660384445073}}\n","2022-11-28 23:48:25,326 INFO EpsilonGreedy average analysis {0: {'count': 44, 'sum': 22772.942857142854, 'min': 93.0, 'max': 869.0, 'mean': 517.5668831168831, 'std': 139.9869507492786}, 1: {'count': 456, 'sum': 239473.78235294123, 'min': 1.0, 'max': 992.0, 'mean': 525.1618034055729, 'std': 164.21660384445073}}\n","2022-11-28 23:48:25,326 INFO EpsilonGreedy average analysis {0: {'count': 44, 'sum': 22772.942857142854, 'min': 93.0, 'max': 869.0, 'mean': 517.5668831168831, 'std': 139.9869507492786}, 1: {'count': 456, 'sum': 239473.78235294123, 'min': 1.0, 'max': 992.0, 'mean': 525.1618034055729, 'std': 164.21660384445073}}\n","INFO:root:EpsilonGreedy maximum analysis {0: {'count': 44, 'sum': 36393, 'min': 93, 'max': 996, 'mean': 827.1136363636364, 'std': 273.11379712069834}, 1: {'count': 456, 'sum': 384687, 'min': 1, 'max': 1000, 'mean': 843.6118421052631, 'std': 281.58361810050593}}\n","2022-11-28 23:48:25,337 INFO EpsilonGreedy maximum analysis {0: {'count': 44, 'sum': 36393, 'min': 93, 'max': 996, 'mean': 827.1136363636364, 'std': 273.11379712069834}, 1: {'count': 456, 'sum': 384687, 'min': 1, 'max': 1000, 'mean': 843.6118421052631, 'std': 281.58361810050593}}\n","2022-11-28 23:48:25,337 INFO EpsilonGreedy maximum analysis {0: {'count': 44, 'sum': 36393, 'min': 93, 'max': 996, 'mean': 827.1136363636364, 'std': 273.11379712069834}, 1: {'count': 456, 'sum': 384687, 'min': 1, 'max': 1000, 'mean': 843.6118421052631, 'std': 281.58361810050593}}\n","INFO:root:Softmax confusion matrix: [array([[  0, 185,   0],\n","       [  0, 161,   0],\n","       [  0, 154,   0]])]\n","2022-11-28 23:48:25,344 INFO Softmax confusion matrix: [array([[  0, 185,   0],\n","       [  0, 161,   0],\n","       [  0, 154,   0]])]\n","2022-11-28 23:48:25,344 INFO Softmax confusion matrix: [array([[  0, 185,   0],\n","       [  0, 161,   0],\n","       [  0, 154,   0]])]\n","INFO:root:Softmax minimum analysis {0: {'count': 0, 'sum': nan, 'min': nan, 'max': nan, 'mean': nan, 'std': nan}, 1: {'count': 500, 'sum': 87629, 'min': 1, 'max': 997, 'mean': 175.258, 'std': 284.4840020739303}}\n","2022-11-28 23:48:25,350 INFO Softmax minimum analysis {0: {'count': 0, 'sum': nan, 'min': nan, 'max': nan, 'mean': nan, 'std': nan}, 1: {'count': 500, 'sum': 87629, 'min': 1, 'max': 997, 'mean': 175.258, 'std': 284.4840020739303}}\n","2022-11-28 23:48:25,350 INFO Softmax minimum analysis {0: {'count': 0, 'sum': nan, 'min': nan, 'max': nan, 'mean': nan, 'std': nan}, 1: {'count': 500, 'sum': 87629, 'min': 1, 'max': 997, 'mean': 175.258, 'std': 284.4840020739303}}\n","INFO:root:Softmax average analysis {0: {'count': 0, 'sum': nan, 'min': nan, 'max': nan, 'mean': nan, 'std': nan}, 1: {'count': 500, 'sum': 262917.9235294118, 'min': 1.0, 'max': 997.0, 'mean': 525.8358470588236, 'std': 166.24446720841726}}\n","2022-11-28 23:48:25,356 INFO Softmax average analysis {0: {'count': 0, 'sum': nan, 'min': nan, 'max': nan, 'mean': nan, 'std': nan}, 1: {'count': 500, 'sum': 262917.9235294118, 'min': 1.0, 'max': 997.0, 'mean': 525.8358470588236, 'std': 166.24446720841726}}\n","2022-11-28 23:48:25,356 INFO Softmax average analysis {0: {'count': 0, 'sum': nan, 'min': nan, 'max': nan, 'mean': nan, 'std': nan}, 1: {'count': 500, 'sum': 262917.9235294118, 'min': 1.0, 'max': 997.0, 'mean': 525.8358470588236, 'std': 166.24446720841726}}\n","INFO:root:Softmax maximum analysis {0: {'count': 0, 'sum': nan, 'min': nan, 'max': nan, 'mean': nan, 'std': nan}, 1: {'count': 500, 'sum': 421205, 'min': 1, 'max': 1000, 'mean': 842.41, 'std': 282.5213937032026}}\n","2022-11-28 23:48:25,362 INFO Softmax maximum analysis {0: {'count': 0, 'sum': nan, 'min': nan, 'max': nan, 'mean': nan, 'std': nan}, 1: {'count': 500, 'sum': 421205, 'min': 1, 'max': 1000, 'mean': 842.41, 'std': 282.5213937032026}}\n","2022-11-28 23:48:25,362 INFO Softmax maximum analysis {0: {'count': 0, 'sum': nan, 'min': nan, 'max': nan, 'mean': nan, 'std': nan}, 1: {'count': 500, 'sum': 421205, 'min': 1, 'max': 1000, 'mean': 842.41, 'std': 282.5213937032026}}\n","INFO:root:Simulation complete\n","2022-11-28 23:48:25,368 INFO Simulation complete\n","2022-11-28 23:48:25,368 INFO Simulation complete\n"]},{"output_type":"stream","name":"stdout","text":["Complete 0.014827291170756022 minutes\n","\n","\n","Random\n","Worst Case Scenario: {0: {'count': 253, 'sum': 50565, 'min': 1, 'max': 997, 'mean': 199.86166007905138, 'std': 317.52001062397824}, 1: {'count': 247, 'sum': 36159, 'min': 9, 'max': 992, 'mean': 146.39271255060729, 'std': 257.06731980252925}}\n","Average Case Scenario: {0: {'count': 253, 'sum': 135145.2, 'min': 10.0, 'max': 997.0, 'mean': 534.1707509881423, 'std': 178.1032292626969}, 1: {'count': 247, 'sum': 125613.22941176473, 'min': 9.0, 'max': 992.0, 'mean': 508.55558466301505, 'std': 166.55558262639775}}\n","Best Case Scenario: {0: {'count': 253, 'sum': 210760, 'min': 10, 'max': 997, 'mean': 833.0434782608696, 'std': 279.43599691254957}, 1: {'count': 247, 'sum': 206391, 'min': 9, 'max': 1000, 'mean': 835.591093117409, 'std': 299.2114853362161}}\n","\n","\n","\n","UCB1\n","Worst Case Scenario: {0: {'count': 0, 'sum': nan, 'min': nan, 'max': nan, 'mean': nan, 'std': nan}, 1: {'count': 500, 'sum': 87629, 'min': 1, 'max': 997, 'mean': 175.258, 'std': 284.4840020739303}}\n","Average Case Scenario: {0: {'count': 0, 'sum': nan, 'min': nan, 'max': nan, 'mean': nan, 'std': nan}, 1: {'count': 500, 'sum': 262917.9235294118, 'min': 1.0, 'max': 997.0, 'mean': 525.8358470588236, 'std': 166.24446720841726}}\n","Best Case Scenario: {0: {'count': 0, 'sum': nan, 'min': nan, 'max': nan, 'mean': nan, 'std': nan}, 1: {'count': 500, 'sum': 421205, 'min': 1, 'max': 1000, 'mean': 842.41, 'std': 282.5213937032026}}\n","\n","\n","\n","ThompsonSampling\n","Worst Case Scenario: {0: {'count': 219, 'sum': 39891, 'min': 1, 'max': 997, 'mean': 182.15068493150685, 'std': 310.8620362870862}, 1: {'count': 281, 'sum': 43647, 'min': 12, 'max': 992, 'mean': 155.32740213523132, 'std': 271.9760945573872}}\n","Average Case Scenario: {0: {'count': 219, 'sum': 117116.40000000002, 'min': 10.0, 'max': 997.0, 'mean': 534.778082191781, 'std': 172.57467835964198}, 1: {'count': 281, 'sum': 147062.29411764705, 'min': 12.0, 'max': 992.0, 'mean': 523.3533598492778, 'std': 162.09026049002696}}\n","Best Case Scenario: {0: {'count': 219, 'sum': 186156, 'min': 10, 'max': 997, 'mean': 850.027397260274, 'std': 270.44515844118195}, 1: {'count': 281, 'sum': 240447, 'min': 12, 'max': 1000, 'mean': 855.6832740213523, 'std': 278.32588500024013}}\n","\n","\n","\n","EpsilonGreedy\n","Worst Case Scenario: {0: {'count': 44, 'sum': 7538, 'min': 1, 'max': 869, 'mean': 171.3181818181818, 'std': 274.8274477432301}, 1: {'count': 456, 'sum': 78663, 'min': 1, 'max': 992, 'mean': 172.5065789473684, 'std': 281.7247006818204}}\n","Average Case Scenario: {0: {'count': 44, 'sum': 22772.942857142854, 'min': 93.0, 'max': 869.0, 'mean': 517.5668831168831, 'std': 139.9869507492786}, 1: {'count': 456, 'sum': 239473.78235294123, 'min': 1.0, 'max': 992.0, 'mean': 525.1618034055729, 'std': 164.21660384445073}}\n","Best Case Scenario: {0: {'count': 44, 'sum': 36393, 'min': 93, 'max': 996, 'mean': 827.1136363636364, 'std': 273.11379712069834}, 1: {'count': 456, 'sum': 384687, 'min': 1, 'max': 1000, 'mean': 843.6118421052631, 'std': 281.58361810050593}}\n","\n","\n","\n","Softmax\n","Worst Case Scenario: {0: {'count': 0, 'sum': nan, 'min': nan, 'max': nan, 'mean': nan, 'std': nan}, 1: {'count': 500, 'sum': 87629, 'min': 1, 'max': 997, 'mean': 175.258, 'std': 284.4840020739303}}\n","Average Case Scenario: {0: {'count': 0, 'sum': nan, 'min': nan, 'max': nan, 'mean': nan, 'std': nan}, 1: {'count': 500, 'sum': 262917.9235294118, 'min': 1.0, 'max': 997.0, 'mean': 525.8358470588236, 'std': 166.24446720841726}}\n","Best Case Scenario: {0: {'count': 0, 'sum': nan, 'min': nan, 'max': nan, 'mean': nan, 'std': nan}, 1: {'count': 500, 'sum': 421205, 'min': 1, 'max': 1000, 'mean': 842.41, 'std': 282.5213937032026}}\n","\n","\n","\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":["Estos gráficos plotean la ganancia acumulada por cada brazo de cada MAB. En este caso, como\n","las recompensas están definidas de antes, las suma de las recompensas de ambos brazos siempre será la misma, por lo que este ejemplo sirve para identificar la razón en la que se elige cada brazo según cada política."],"metadata":{"id":"T27ppI_3TnsY"}},{"cell_type":"code","source":[],"metadata":{"id":"roDE512XGqqw"},"execution_count":null,"outputs":[]}]}